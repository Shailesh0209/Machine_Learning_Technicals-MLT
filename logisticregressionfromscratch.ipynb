{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df086d3b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:44.647401Z",
     "iopub.status.busy": "2022-05-31T09:52:44.646483Z",
     "iopub.status.idle": "2022-05-31T09:52:44.661839Z",
     "shell.execute_reply": "2022-05-31T09:52:44.661026Z"
    },
    "papermill": {
     "duration": 0.0303,
     "end_time": "2022-05-31T09:52:44.664210",
     "exception": false,
     "start_time": "2022-05-31T09:52:44.633910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a4920",
   "metadata": {
    "papermill": {
     "duration": 0.007944,
     "end_time": "2022-05-31T09:52:44.680133",
     "exception": false,
     "start_time": "2022-05-31T09:52:44.672189",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### x_W5/L4-mlt-dip-iitm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d85c00",
   "metadata": {
    "papermill": {
     "duration": 0.007711,
     "end_time": "2022-05-31T09:52:44.695623",
     "exception": false,
     "start_time": "2022-05-31T09:52:44.687912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic regression Implementation\n",
    "\n",
    "- Logistic regression is the workhorse of machine learning\n",
    "\n",
    "- Before deep learning era, logistic regression was the default choice for solving real life classification problems with hundreds of thousand of features.\n",
    "\n",
    "- It works in binary, multi-class and multi-label classification set ups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2040b3a4",
   "metadata": {
    "papermill": {
     "duration": 0.00739,
     "end_time": "2022-05-31T09:52:44.710762",
     "exception": false,
     "start_time": "2022-05-31T09:52:44.703372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "813692c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:44.728189Z",
     "iopub.status.busy": "2022-05-31T09:52:44.727420Z",
     "iopub.status.idle": "2022-05-31T09:52:45.910794Z",
     "shell.execute_reply": "2022-05-31T09:52:45.909823Z"
    },
    "papermill": {
     "duration": 1.194814,
     "end_time": "2022-05-31T09:52:45.913352",
     "exception": false,
     "start_time": "2022-05-31T09:52:44.718538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex # Imported for proper rendering of latex in colab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import for generating plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(1234)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee014382",
   "metadata": {
    "papermill": {
     "duration": 0.007523,
     "end_time": "2022-05-31T09:52:45.928792",
     "exception": false,
     "start_time": "2022-05-31T09:52:45.921269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As a good practice, set the random seed in order to reproduce same results across different runs of this colab.\n",
    "> We can set random seed to any other number of our choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a104c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:45.946784Z",
     "iopub.status.busy": "2022-05-31T09:52:45.945843Z",
     "iopub.status.idle": "2022-05-31T09:52:45.950348Z",
     "shell.execute_reply": "2022-05-31T09:52:45.949650Z"
    },
    "papermill": {
     "duration": 0.015777,
     "end_time": "2022-05-31T09:52:45.952335",
     "exception": false,
     "start_time": "2022-05-31T09:52:45.936558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d173df",
   "metadata": {
    "papermill": {
     "duration": 0.0076,
     "end_time": "2022-05-31T09:52:45.967976",
     "exception": false,
     "start_time": "2022-05-31T09:52:45.960376",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfb5cbf",
   "metadata": {
    "papermill": {
     "duration": 0.007195,
     "end_time": "2022-05-31T09:52:45.982906",
     "exception": false,
     "start_time": "2022-05-31T09:52:45.975711",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c28b2",
   "metadata": {
    "papermill": {
     "duration": 0.007225,
     "end_time": "2022-05-31T09:52:45.997950",
     "exception": false,
     "start_time": "2022-05-31T09:52:45.990725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- As we know, the logistic regression classifier calculates the probability of a sample, represented with a feature vector X, belonging to class 1: Pr(y = 1|x).\n",
    "- It has two steps:\n",
    "    1. Linear combination of features and\n",
    "    2. Sigmoid activation\n",
    "    \n",
    "> Let's apply these steps on a single example to calculate its probability of belonging to class 1:\n",
    "    1. The first step performs linear combination of features and obtain z = $W^TX$\n",
    "    2. The second step applies sigmoid or logistic activation on z to obtain the probability:\n",
    "    Pr(y = 1|X) = sigmoid(Z) = 1/(1 + $e^-z$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c8047",
   "metadata": {
    "papermill": {
     "duration": 0.007333,
     "end_time": "2022-05-31T09:52:46.012980",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.005647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The vectorized form enables us to compute probabilities for several examples all at once as follow:\n",
    "    \n",
    "1. By vectorizing linear combination of features leading to efficient computation: Znx1 = (Xnxm)Wmx1\n",
    "    \n",
    "     * The resulting linear combination z is a vector with n components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b20d337",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.031146Z",
     "iopub.status.busy": "2022-05-31T09:52:46.030507Z",
     "iopub.status.idle": "2022-05-31T09:52:46.034941Z",
     "shell.execute_reply": "2022-05-31T09:52:46.034131Z"
    },
    "papermill": {
     "duration": 0.016143,
     "end_time": "2022-05-31T09:52:46.037041",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.020898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's implement linear combination in vectorized form\n",
    "def linear_combination(X:np.ndarray, w:np.ndarray) ->np.ndarray:\n",
    "    return X@w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4424c0",
   "metadata": {
    "papermill": {
     "duration": 0.007169,
     "end_time": "2022-05-31T09:52:46.051915",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.044746",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2. By vectorizing sigmoid or logistic activation to obtain a vector of probability or activation:\n",
    "    Pr(y=1|X)nx1 = sigmoid(Znx1)\n",
    "    * The sigmoid function is applied on the vecctor z with n components and the result is a probability or an activation vector with n components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ab57d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.069201Z",
     "iopub.status.busy": "2022-05-31T09:52:46.068579Z",
     "iopub.status.idle": "2022-05-31T09:52:46.073409Z",
     "shell.execute_reply": "2022-05-31T09:52:46.072635Z"
    },
    "papermill": {
     "duration": 0.01562,
     "end_time": "2022-05-31T09:52:46.075384",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.059764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's implement sigmoid function in a vectorized form\n",
    "def sigmoid(z:np.ndarray) -> np.ndarray:\n",
    "    '''Calculates sigmoid of linear combinatin of features z.\n",
    "    Args:\n",
    "        z: list of floats\n",
    "        \n",
    "    Returns:\n",
    "        List of output of sigmoid function\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833b920e",
   "metadata": {
    "papermill": {
     "duration": 0.007489,
     "end_time": "2022-05-31T09:52:46.090397",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.082908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Further we apply vectorized prediction or inference function on activations to obtain a class label. Specifically, if activation or probability > threshold, then we label the sample with class 1, or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1c42e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.107679Z",
     "iopub.status.busy": "2022-05-31T09:52:46.107056Z",
     "iopub.status.idle": "2022-05-31T09:52:46.112107Z",
     "shell.execute_reply": "2022-05-31T09:52:46.111497Z"
    },
    "papermill": {
     "duration": 0.015907,
     "end_time": "2022-05-31T09:52:46.114076",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.098169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X:np.ndarray, w:np.ndarray, threshold:float) -> np.ndarray:\n",
    "    '''Predicts class label for samples.\n",
    "    \n",
    "    The samples are represented with a bunch of features and are \n",
    "    presented in form of a feature matrix X. The class label is predicted as follows:\n",
    "    * If sigmoid(Xw) > threshold, the sample is labeled with class 1.\n",
    "        * else class 0.\n",
    "        \n",
    "    Args:\n",
    "        X: feature vector of shape(n, m)\n",
    "        w: weight vector of shape(m,)\n",
    "        threshold: probability threshold for classification.\n",
    "    Returns:\n",
    "        A list of class labels of shape (n,)\n",
    "    \n",
    "    '''\n",
    "    return np.where(sigmoid(linear_combination(X, w)) > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33528e",
   "metadata": {
    "papermill": {
     "duration": 0.007668,
     "end_time": "2022-05-31T09:52:46.129531",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.121863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's label a couple of samples through the code that we have written so far-\n",
    "- Two samples-each with two features-np.array([[1, 20, 2], [1, 2, 2]]) where the first one is a dummy feature set to 1 corresponding to the bias.\n",
    "- weight vector: np.array([-1, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7d1dd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.146576Z",
     "iopub.status.busy": "2022-05-31T09:52:46.145936Z",
     "iopub.status.idle": "2022-05-31T09:52:46.154733Z",
     "shell.execute_reply": "2022-05-31T09:52:46.153886Z"
    },
    "papermill": {
     "duration": 0.019724,
     "end_time": "2022-05-31T09:52:46.156891",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.137167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature  matrix:  (2, 3)\n",
      "Shape of weight  vector:  (3,)\n",
      "Shape of output is;  (2,)\n",
      "The class label vector is:  [1 1]\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = np.array([[1, 20, 2], [1, 2, 2]])\n",
    "weight_vector = np.array([-1, 0, 1])\n",
    "\n",
    "print(\"Shape of feature  matrix: \", feature_matrix.shape)\n",
    "print(\"Shape of weight  vector: \", weight_vector.shape)\n",
    "\n",
    "class_labels = predict(feature_matrix, weight_vector, 0.5)\n",
    "\n",
    "print(\"Shape of output is; \", class_labels.shape)\n",
    "print(\"The class label vector is: \", class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb1ea28",
   "metadata": {
    "papermill": {
     "duration": 0.007642,
     "end_time": "2022-05-31T09:52:46.174707",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.167065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Both the samples are labelled with class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff1d48",
   "metadata": {
    "papermill": {
     "duration": 0.007555,
     "end_time": "2022-05-31T09:52:46.189827",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.182272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d5192d",
   "metadata": {
    "papermill": {
     "duration": 0.007637,
     "end_time": "2022-05-31T09:52:46.205271",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.197634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we will implement binary cross entropy(BCE) loss with regularization. The base loss without regularization can be obtained by setting the regularization rate $\\lambda$ to 0.\n",
    "\n",
    "The generic form of loss is as follows:\n",
    " BCE = BCE on training examples + $\\lambda$regularization penalty\n",
    " \n",
    "Note that regularization rate $\\lambda$ controls the amount of regularization penalty to be used.\n",
    "\n",
    "We use $L_2$ and $L_1$ regularization in logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e26c9",
   "metadata": {
    "papermill": {
     "duration": 0.007511,
     "end_time": "2022-05-31T09:52:46.221117",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.213606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In order to write the loss in vectorized form, we will first calculate the term inside summation in vectorized form;\n",
    "    e = ylog(sigmoid(Xw)) + (1-y)log(1 - sigmoid(Xw))\n",
    "    \n",
    "With this, the loss becomes: J(w) = $-1^T$1xn(**e**nx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26eb996",
   "metadata": {
    "papermill": {
     "duration": 0.007614,
     "end_time": "2022-05-31T09:52:46.236537",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.228923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Exercise; verify this equality.\n",
    "Adding $L_2$ penalty, we get: J(w) = -1^T(e) + $\\lambda$$w^T$w\n",
    "\n",
    "Adding $L_1$ penalty, we get: J(w) = -1^T(e) + $\\lambda$$1^T$|w|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019a8141",
   "metadata": {
    "papermill": {
     "duration": 0.007311,
     "end_time": "2022-05-31T09:52:46.251310",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.243999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The loss function implements vectorized loss calculation with actual label vector, activation vector, weight vector, and $L_1$ and $L_2$ regularization rates\n",
    "\n",
    "\n",
    "The loss function returns a scalar quantity that denotes the loss on all training examples for a particular choice of the weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d9b7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.268868Z",
     "iopub.status.busy": "2022-05-31T09:52:46.268194Z",
     "iopub.status.idle": "2022-05-31T09:52:46.273767Z",
     "shell.execute_reply": "2022-05-31T09:52:46.273025Z"
    },
    "papermill": {
     "duration": 0.016669,
     "end_time": "2022-05-31T09:52:46.275704",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.259035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss(y, sigmoid_vector, weight_vector, l1_reg_rate, l2_reg_rate):\n",
    "    return (-1 * (np.sum(y * np.log(sigmoid_vector) + (1-y) * np.log(1-sigmoid_vector)))\n",
    "             + l2_reg_rate * np.dot(np.transpose(weight_vector), weight_vector)\n",
    "            + l1_reg_rate * np.sum(np.abs(weight_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31f3fd",
   "metadata": {
    "papermill": {
     "duration": 0.007297,
     "end_time": "2022-05-31T09:52:46.290799",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.283502",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Optimization\n",
    "\n",
    "Next, we will implement optimization. For that we will make use of iterative optimization techniques like gradient descent(GD), mini-batch gradient descent(MBGD) or stochastic gradient descent(SGD).\n",
    "\n",
    "We will use GD implementation from linear regression.\n",
    "\n",
    "We need to modify gradient update rule that is suitable for logistic regression loss:\n",
    "    * STEP 1: Calculate gradient of loss function and\n",
    "    * STEP 2: Scale the gradient with learning rate and use it for updating the weight vector.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641c8228",
   "metadata": {
    "papermill": {
     "duration": 0.007454,
     "end_time": "2022-05-31T09:52:46.306033",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.298579",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Gradient of loss function\n",
    "\n",
    "The gradient of loss function can be calculated (in vecctorized form) as follow:\n",
    "d(J(w)/dw = $X^T$(sigmoid(Xw)-y) + $\\lambda$w\n",
    "\n",
    "It is imlemented with `calculate_gradient` fuction that takes feature matrix X, label vector y, weight vector w and regularization rate $\\lambda$ as arguments and effieciently calculates gradient of loss function w.r.t. the weight vector in vectorized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20ec1d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.323634Z",
     "iopub.status.busy": "2022-05-31T09:52:46.322942Z",
     "iopub.status.idle": "2022-05-31T09:52:46.328652Z",
     "shell.execute_reply": "2022-05-31T09:52:46.327803Z"
    },
    "papermill": {
     "duration": 0.016878,
     "end_time": "2022-05-31T09:52:46.330752",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.313874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(X:np.ndarray, y:np.ndarray, w:np.ndarray, reg_rate:float) -> np.ndarray:\n",
    "    '''Calculates gradient of loss function w.r.t. weight vector on training set.\n",
    "    The gradient is calculated with the following vectorized operation:\n",
    "        np.transpose(X)(sigmoid(Xw) - y) + \\lambda w\n",
    "    Args:\n",
    "        X: Feature matrix for training data.\n",
    "        y: label vector for training data.\n",
    "        reg_rate: regularization rate\n",
    "    Returns:\n",
    "        A vector of gradients\n",
    "        \n",
    "        \n",
    "        '''\n",
    "    return np.transpose(X)@(sigmoid(linear_combination(X, w)) - y) + reg_rate * w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236873fd",
   "metadata": {
    "papermill": {
     "duration": 0.008168,
     "end_time": "2022-05-31T09:52:46.346669",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.338501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As part of the implementation, we store loss and weight vectors in each GD step as a class member variable.\n",
    "- The step-wise loss is used for plotting a learning curve in order to ensure that the model is training as expected.\n",
    "\n",
    "- The step-wise weight vector is useful in studying the trajectory of gradient descent in the loss landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a052b02",
   "metadata": {
    "papermill": {
     "duration": 0.007746,
     "end_time": "2022-05-31T09:52:46.362375",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.354629",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic regression class implementation\n",
    "\n",
    "We combine these different components into a single python class with name `LogisticRegression`.\n",
    "\n",
    "It has the following class member variables:\n",
    "   1. Weight vector\n",
    "   2. Loss and weight vector in each GD step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "682838cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-31T09:52:46.380065Z",
     "iopub.status.busy": "2022-05-31T09:52:46.379421Z",
     "iopub.status.idle": "2022-05-31T09:52:46.396877Z",
     "shell.execute_reply": "2022-05-31T09:52:46.395941Z"
    },
    "papermill": {
     "duration": 0.029062,
     "end_time": "2022-05-31T09:52:46.399218",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.370156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    \"\"\"Logistic regression model.\n",
    "    y = sigmoid(X @ w)\n",
    "    \"\"\"\n",
    "    def set_weight_vector(self, w):\n",
    "        self.w\n",
    "        \n",
    "    # Let's implement linear combination in vectorized form\n",
    "    def linear_combination(self, X:np.ndarray) ->np.ndarray:\n",
    "        return X@self.w\n",
    "    \n",
    "    def sigmoid(self, z:np.ndarray):\n",
    "        \"\"\"Return probability of input belonging class 1.\n",
    "        Args:\n",
    "            z: (n, ) np.ndarray\n",
    "        Returns:\n",
    "            sigmoid activation vector(n, ) np.ndarray\n",
    "        \n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def activation(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates sigmoid activation for logistic regressin.\n",
    "        The sigmoid activation is calculated with the following vectorized form:\n",
    "            act = sigmoid(Xw)\n",
    "            \n",
    "        Args:\n",
    "            X: feature matrix with shape (n, m)\n",
    "        Returns:\n",
    "            activation vector with the shape (n,)'''\n",
    "        return self.sigmoid(self.linear_combination(X))\n",
    "    \n",
    "    def predict(self, X:np.ndarray, threshold: float = 0.5):\n",
    "        \"\"\"Classify input data.\n",
    "        Args:\n",
    "            x : (N, D) np.ndarray\n",
    "            threshold : float, optional\n",
    "                threshold of binary classification (default is 0.5)\n",
    "        Returns:\n",
    "            (N, ) np.ndarray \n",
    "        \"\"\"\n",
    "        return (self.activation(X) > threshold).astype(int)\n",
    "    \n",
    "    def loss(self, X:np.ndarray, y:np.ndarray, reg_rate:float) -> float:\n",
    "        '''Calculates binary cross entropy loss on training set.\n",
    "        \n",
    "        '''\n",
    "        predicted_prob = self.activation(X)\n",
    "        return (-1 * (np.sum(y * np.log(predicted_prob) + \n",
    "                             (1-y) * np.log(1 - predicted_prob))) +\n",
    "                            reg_rate * np.dot(np.transpose(self.w), self.w))\n",
    "    \n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray,\n",
    "                          reg_rate:float) -> np.ndarray:\n",
    "        '''Calculates gradients of loss function w.r.t. weight vector on training set.\n",
    "        \n",
    "        Returns:\n",
    "            A vector of gradients.'''\n",
    "        return np.transpose(X)@(self.activation(X) -y) + reg_rate * self.w\n",
    "    \n",
    "    def update_weights(self, grad:np.ndarray, lr:float) -> np.ndarray:\n",
    "        '''Updates the weights based on the gradient of loss function.\n",
    "        \n",
    "        Weight updates are carried out with the following formula:\n",
    "            w_new := w_old -lr * grad\n",
    "            \n",
    "        Args:\n",
    "            2. grad: gradient of loss w.r.t. w\n",
    "            3. lr: learning rate\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "            '''\n",
    "        return (self.w - lr*grad)\n",
    "    \n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate:float) -> np.ndarray:\n",
    "        '''Estimates parameters of linear regression model through gradient descent.\n",
    "        Args:\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data.\n",
    "            num_epochs: Number of training steps\n",
    "            lr: learning rate\n",
    "            reg_rate: regularization rate\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector'''\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "        for i in np.arange(0, num_epochs):\n",
    "            dJdW = self.calculate_gradient(X, y, reg_rate)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X, y, reg_rate))\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb43b054",
   "metadata": {
    "papermill": {
     "duration": 0.007292,
     "end_time": "2022-05-31T09:52:46.414339",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.407047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this section, we implemented binary logistic regression classifier from scratch. First we implemented all its components in vectorized form and then clubbed all of them together in a python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9f33a5",
   "metadata": {
    "papermill": {
     "duration": 0.007408,
     "end_time": "2022-05-31T09:52:46.429821",
     "exception": false,
     "start_time": "2022-05-31T09:52:46.422413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.089858,
   "end_time": "2022-05-31T09:52:47.159271",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-05-31T09:52:34.069413",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
